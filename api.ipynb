{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\L580\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\L580\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import library for ReGex, SQLite, Pandas, Numpy, and joblib\n",
    "import re\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Import library for Tokenize, Stemming, and Stopwords\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as stopwords_scratch\n",
    "\n",
    "# Import library for SKLearn Model Sentiment Analysis\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import library for Tensorflow Model Sentiment Analysis\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM,Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "# Import library for Flask\n",
    "from flask import Flask, jsonify\n",
    "from flask import request, make_response\n",
    "from flask_swagger_ui import get_swaggerui_blueprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swagger UI Definition\n",
    "app = Flask(__name__)\n",
    "app.config['JSON_SORT_KEYS'] = False # Mengubah order JSON menjadi urutan yang benar\n",
    "SWAGGER_URL = '/swagger'\n",
    "API_URL = '/static/restapi_sentiment.yml'\n",
    "SWAGGERUI_BLUEPRINT = get_swaggerui_blueprint(\n",
    "    SWAGGER_URL,\n",
    "    API_URL,\n",
    "    config={\n",
    "        'app_name': \"DSental (Data Sentiment Analysis)\"\n",
    "    }\n",
    ")\n",
    "app.register_blueprint(SWAGGERUI_BLUEPRINT, url_prefix=SWAGGER_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>warung ini dimiliki oleh pengusaha pabrik tahu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mohon ulama lurus dan k212 mmbri hujjah partai...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lokasi strategis di jalan sumatera bandung . t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>betapa bahagia nya diri ini saat unboxing pake...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>duh . jadi mahasiswa jangan sombong dong . kas...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10993</th>\n",
       "      <td>f - demokrat dorong upaya kemandirian energi n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10994</th>\n",
       "      <td>tidak bosan</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10996</th>\n",
       "      <td>enak rasa masakan nya apalagi kepiting yang me...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10998</th>\n",
       "      <td>pagi pagi di tol pasteur sudah macet parah , b...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10999</th>\n",
       "      <td>meskipun sering belanja ke yogya di riau junct...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10933 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text     label\n",
       "0      warung ini dimiliki oleh pengusaha pabrik tahu...  positive\n",
       "1      mohon ulama lurus dan k212 mmbri hujjah partai...   neutral\n",
       "2      lokasi strategis di jalan sumatera bandung . t...  positive\n",
       "3      betapa bahagia nya diri ini saat unboxing pake...  positive\n",
       "4      duh . jadi mahasiswa jangan sombong dong . kas...  negative\n",
       "...                                                  ...       ...\n",
       "10993  f - demokrat dorong upaya kemandirian energi n...   neutral\n",
       "10994                                        tidak bosan  positive\n",
       "10996  enak rasa masakan nya apalagi kepiting yang me...  positive\n",
       "10998  pagi pagi di tol pasteur sudah macet parah , b...  negative\n",
       "10999  meskipun sering belanja ke yogya di riau junct...  positive\n",
       "\n",
       "[10933 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to db & csv\n",
    "conn = sqlite3.connect(r'D:\\Data Science - Binar Academy Wave 2\\PLATINUM\\Challange\\challenge_platinum\\DATA\\output (1).db', check_same_thread=False)\n",
    "df_alay = pd.read_csv(r'D:\\Data Science - Binar Academy Wave 2\\PLATINUM\\Challange\\challenge_platinum\\DATA\\new_kamusalay.csv', names=['alay','cleaned'], encoding ='latin-1')\n",
    "data_raw = pd.read_csv(r'D:\\Data Science - Binar Academy Wave 2\\PLATINUM\\Challange\\challenge_platinum\\train_preprocess.tsv', sep='\\t',names=['text','label'])\n",
    "data_raw.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x26dc4673340>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define and Execute query for unexistence data tables\n",
    "# Tables will contain fields with dirty text (text & file) and cleaned text (text & file)\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS data_text_sk (text_id INTEGER PRIMARY KEY AUTOINCREMENT, text varchar(255), sentiment varchar(255));''')\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS data_file_sk (text_id INTEGER PRIMARY KEY AUTOINCREMENT, text varchar(255), sentiment varchar(255));''')\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS data_text_tf (text_id INTEGER PRIMARY KEY AUTOINCREMENT, text varchar(255), sentiment varchar(255));''')\n",
    "conn.execute('''CREATE TABLE IF NOT EXISTS data_file_tf (text_id INTEGER PRIMARY KEY AUTOINCREMENT, text varchar(255), sentiment varchar(255));''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Stopwords\n",
    "list_stopwords = stopwords_scratch.words('indonesian')\n",
    "list_stopwords_en = stopwords_scratch.words('english')\n",
    "list_stopwords.extend(list_stopwords_en)\n",
    "list_stopwords.extend(['ya','yg','ga','yuk','dah','baiknya', 'berkali', 'kali', 'kurangnya', 'mata', 'olah', 'sekurang', 'setidak', 'tama', 'tidaknya'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add External Stopwords\n",
    "f = open(\"D:\\Data Science - Binar Academy Wave 2\\PLATINUM\\Challange\\challenge_platinum\\stopwords\\delete_from_stopword.txt\", \"r\")\n",
    "stopword_external = []\n",
    "for line in f:\n",
    "    stripped_line = line.strip()\n",
    "    line_list = stripped_line.split()\n",
    "    stopword_external.append(line_list[0])\n",
    "f.close()\n",
    "list_stopwords.extend(stopword_external)\n",
    "stopwords = list_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Function for Cleansing Process\n",
    "def lowercase(text): # Change uppercase characters to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "def special(text): # Remove all the special characters\n",
    "    text = re.sub(r'\\W', ' ',str(text), flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def single(text): # remove all single characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def singlestart(text): # Remove single characters from the start\n",
    "    text = re.sub(r'\\^[a-zA-Z]\\s+', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def mulspace(text): # Substituting multiple spaces with single space\n",
    "    text = re.sub(r'\\s+', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def rt(text): # Remove RT\n",
    "    text = re.sub(r'rt @\\w+: ', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def prefixedb(text): # Removing prefixed 'b'\n",
    "    text = re.sub(r'^b\\s+', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "def misc(text): # Remove URL, Mention, Hashtag, user, Line, and Tab\n",
    "    text = re.sub(r'((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))|([#@]\\S+)|user|\\n|\\t', ' ', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "\n",
    "alay_mapping = dict(zip(df_alay['alay'], df_alay['cleaned'])) # Mapping for kamusalay\n",
    "def alay(text): # Remove by replacing 'alay' words\n",
    "    wordlist = text.split()\n",
    "    text_alay = [alay_mapping.get(x,x) for x in wordlist]\n",
    "    clean_alay = ' '.join(text_alay)\n",
    "    return clean_alay\n",
    "\n",
    "def stopwrds(text): # Stopwords fuction\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in stopwords]\n",
    "    output_sw = ' '.join(tokens_without_sw)\n",
    "    return output_sw\n",
    "\n",
    "# Function for text cleansing\n",
    "def cleaning(text):\n",
    "    text = lowercase(text)\n",
    "    text = special(text)\n",
    "    text = single(text)\n",
    "    text = singlestart(text)\n",
    "    text = mulspace(text)\n",
    "    text = rt(text)\n",
    "    text = prefixedb(text)\n",
    "    text = misc(text)\n",
    "    text = alay(text)\n",
    "    text = stopwrds(text)\n",
    "    return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLEARN NEURAL NETWORK MODEL ANALYSIS SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SKLearn Model\n",
    "f1 = joblib.load(r'D:\\Data Science - Binar Academy Wave 2\\PLATINUM\\Challange\\challenge_platinum\\sklearn\\score.pkl')\n",
    "clf = joblib.load(r'D:\\Data Science - Binar Academy Wave 2\\PLATINUM\\Challange\\challenge_platinum\\sklearn\\model.pkl')\n",
    "vectorizer = joblib.load(r'D:\\Data Science - Binar Academy Wave 2\\PLATINUM\\Challange\\challenge_platinum\\sklearn\\vectorizer.pkl')\n",
    "\n",
    "# Function for CSV SKLearn Analysis\n",
    "def sentiment_csv_nn(input_file):\n",
    "    column = input_file.iloc[:, 0]\n",
    "    print(column)\n",
    "\n",
    "    for data_file in column: # Define and execute query for insert cleaned text and sentiment to sqlite database\n",
    "        data_clean = cleaning(data_file)\n",
    "        sent = clf.predict(vectorizer.transform([data_clean]).toarray())\n",
    "        query = \"insert into data_file_sk (text,sentiment) values (?, ?)\"\n",
    "        val = (data_clean,str(sent))\n",
    "        conn.execute(query, val)\n",
    "        conn.commit()\n",
    "        print(data_file)\n",
    "\n",
    "# Create Homepage\n",
    "@app.route('/', methods=['GET'])\n",
    "def get():\n",
    "    return \"Welcome to DSental!\"\n",
    "\n",
    "# Endpoint for Text Analysis SKLearn\n",
    "# Input text to analyze\n",
    "@app.route('/text_sklearn', methods=['POST'])\n",
    "def text_sentiment_sk():\n",
    "\n",
    "    # Get text from user\n",
    "    input_text = str(request.form['text'])\n",
    "\n",
    "    # Cleaning text\n",
    "    output_text = cleaning(input_text)\n",
    "\n",
    "    # Model Prediction for Sentiment Analysis\n",
    "    sent = clf.predict(vectorizer.transform([output_text]).toarray())\n",
    "\n",
    "    # Define and execute query for insert cleaned text and sentiment to sqlite database\n",
    "    query = \"insert into data_text_sk (text,sentiment) values (?, ?)\"\n",
    "    val = (output_text,str(sent))\n",
    "    conn.execute(query, val)\n",
    "    conn.commit()\n",
    "\n",
    "    # Define API response\n",
    "    json_response = {\n",
    "        'description': \"Analysis Sentiment Success!\",\n",
    "        'F1 on test set': f1,\n",
    "        'text' : output_text,\n",
    "        'sentiment' : str(sent)\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "# Endpoint for File Analysis SKLearn\n",
    "@app.route('/file_sklearn', methods=['POST'])\n",
    "def file_sentiment_sk():\n",
    "\n",
    "    # Get file\n",
    "    file = request.files['file']\n",
    "    try:\n",
    "            datacsv = pd.read_csv(file, encoding='iso-8859-1')\n",
    "    except:\n",
    "            datacsv = pd.read_csv(file, encoding='utf-8')\n",
    "    \n",
    "    # Cleaning file\n",
    "    sentiment_csv_nn(datacsv)\n",
    "\n",
    "    # Define API response\n",
    "    select_data = conn.execute(\"SELECT * FROM data_file_sk\")\n",
    "    conn.commit\n",
    "    data = [\n",
    "        dict(text_id=row[0], text=row[1], sentiment=row[2])\n",
    "    for row in select_data.fetchall()\n",
    "    ]\n",
    "    \n",
    "    return jsonify(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TENSORFLOW LSTM MODEL ANALYSIS SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tensorflow Model\n",
    "model = load_model(r'D:\\Data Science - Binar Academy Wave 2\\PLATINUM\\Challange\\challenge_platinum\\tensorflow\\model.h5')\n",
    "tokenizer = joblib.load(r'D:\\Data Science - Binar Academy Wave 2\\PLATINUM\\Challange\\challenge_platinum\\tensorflow\\tokenizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction\n",
    "# Create Function for Sentiment Prediction\n",
    "def predict_sentiment(text):\n",
    "    sentiment_tf = ['negative', 'neutral', 'positive']\n",
    "    text = cleaning(text)\n",
    "    tw = tokenizer.texts_to_sequences([text])\n",
    "    tw = pad_sequences(tw, maxlen=200)\n",
    "    prediction = model.predict(tw)\n",
    "    polarity = np.argmax(prediction[0])\n",
    "    return sentiment_tf[polarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for CSV Tensorflow Analysis\n",
    "def sentiment_csv_tf(input_file):\n",
    "    column = input_file.iloc[:, 0]\n",
    "    print(column)\n",
    "\n",
    "    for data_file in column: # Define and execute query for insert cleaned text and sentiment to sqlite database\n",
    "        data_clean = cleaning(data_file)\n",
    "        sent = predict_sentiment(data_clean)\n",
    "        query = \"insert into data_file_tf (text,sentiment) values (?, ?)\"\n",
    "        val = (data_clean,sent)\n",
    "        conn.execute(query, val)\n",
    "        conn.commit()\n",
    "        print(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint for Text Analysis TensorFlow\n",
    "# Input text to analyze\n",
    "@app.route('/text_tensorflow', methods=['POST'])\n",
    "def text_sentiment_tf():\n",
    "\n",
    "    # Get text from user\n",
    "    input_text = str(request.form['text'])\n",
    "\n",
    "    # Cleaning text\n",
    "    output_text = cleaning(input_text)\n",
    "\n",
    "    # Model Prediction for Sentiment Analysis\n",
    "    output_sent = predict_sentiment(output_text)\n",
    "\n",
    "    # Define and execute query for insert cleaned text and sentiment to sqlite database\n",
    "    query = \"insert into data_text_tf (text,sentiment) values (?, ?)\"\n",
    "    val = (output_text,output_sent)\n",
    "    conn.execute(query, val)\n",
    "    conn.commit()\n",
    "\n",
    "    # Define API response\n",
    "    json_response = {\n",
    "        'description': \"Analysis Sentiment Success!\",\n",
    "        'text' : output_text,\n",
    "        'sentiment' : output_sent\n",
    "    }\n",
    "    response_data = jsonify(json_response)\n",
    "    return response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Endpoint for File Analysis TensorFlow\n",
    "@app.route('/file_tensorflow', methods=['POST'])\n",
    "def file_sentiment_tf():\n",
    "\n",
    "    # Get file\n",
    "    file = request.files['file']\n",
    "    try:\n",
    "            datacsv = pd.read_csv(file, encoding='iso-8859-1')\n",
    "    except:\n",
    "            datacsv = pd.read_csv(file, encoding='utf-8')\n",
    "\n",
    "    # Cleaning file\n",
    "    sentiment_csv_tf(datacsv)\n",
    "\n",
    "    # Define API response\n",
    "    select_data = conn.execute(\"SELECT * FROM data_file_tf\")\n",
    "    conn.commit\n",
    "    data = [\n",
    "        dict(text_id=row[0], text=row[1], sentiment=row[2])\n",
    "    for row in select_data.fetchall()\n",
    "    ]\n",
    "    \n",
    "    return jsonify(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:1234\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "# handling error\n",
    "@app.errorhandler(404)\n",
    "def error_404(error):\n",
    "    return make_response(jsonify({'error': 'not found'}), 404)\n",
    "\n",
    "@app.errorhandler(500)\n",
    "def error_500(error):\n",
    "    return make_response(jsonify({'error': 'server error'}), 500)\n",
    "\n",
    "# run server\n",
    "if __name__ == '__main__':    \n",
    "    app.run(port = 1234, debug=True,use_reloader=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5 (tags/v3.10.5:f377153, Jun  6 2022, 16:14:13) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9a0684188baa8478e4aab5739f9a6ca9e6188f8492b9b67656bb38dc99a20e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
